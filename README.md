# The Explainability of a Black-Box Malware Detection Model Using Tabular and NLP-Based Methods
Authors: Junel Alje B. Isanan and Dr. Rachel Edita O. Roxas Verified: by REOR, onJuly 06, 2024

The exponential surge in malware types, from 100 million in 2012 to 700 million in 2018, poses a significant threat to the cybersecurity sector. While deep learning models have demonstrated impressive accuracy in malware detection and classification, even exceeding 99\% in recent studies, the underlying features driving these predictions remain elusive. This lack of transparency hinders user trust in the models' decisions. To address this, this research explores the use of XAI tools to analyze the inner workings of black-box malware detection models trained on tabular and NLP-based data from the EMBER dataset. By uncovering the influential features in single data points, this study aims to enhance model transparency, foster trust in malware detection predictions, and pave the way for improved detection methods and more effective threat analysis in the ever-evolving landscape of cybersecurity.

Keywords: AI, malware, data science, malware data science, malware analysis, XAI, explainable AI, cybersecurity, deep learning, MLP, neural network, black-box models
